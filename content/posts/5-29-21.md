+++ 
draft = true
date = 2021-05-29T01:41:34-06:00
title = "Turning the Kubernetes API Server into a Container Registry"
slug = "k8s-api-server-container-registry" 
tags = []
categories = []
+++

I'm three cups of coffee in this morning and can't think of a good introduction
to this post. I've gone a few different directions already. I started off with
some meta statement about how programmers don't "really understand" how software
works these days, but I threw that right out like the gatekeeping piece of
garbage that it was. Then, I hit my ["engage thought leadership"
button](https://tenor.com/XabE.gif) and started opining about the usefulness of
flexible interfaces. I actually still have that opening paragraph sitting here
in my editor... it's actually pretty good I think. Okay, maybe I will start with
that one. But I guess at this point we have already started haven't we? Hm okay,
you're right, I'll save that intro for another day.

I'll cut to the chase, today we are going to use the `Pod` proxy subresource in
the Kubernetes API to access an OCI image registry[^1] running in our cluster.
Why? I'm glad you asked![^3] Maybe I'll start with why I even considered doing
this in the first place. My primary motivation was fairly straightforward: I
wanted to have the `docker push` experience, but by pointing it at my
`kubeconfig`. If my goal is to get images into a cluster[^4] and I already have
a way to locate, authenticate to, and set up
[authorization](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)
for that cluster, I shouldn't need to jump through additional hoops.

So what is holding me back from pushing directly to the registry I am running in
the cluster? The primary issue we need to address is connectivity. Assuming I
don't want to dangle my registry endpoints out on the public internet, I need to
set up a way to privately expose the registry. There are many ways to accomplish
this, but most of them require a sufficient level of complexity, such as setting
up a [jump server](https://en.wikipedia.org/wiki/Jump_server), or managing some
form of [VPN](https://en.wikipedia.org/wiki/Virtual_private_network). You may
already have some of this infrastructure in place if your control plane itself
is only privately accessible[^5], but [creating a
`Service`](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types)
to expose the registry `Pod` internally either via `ClusterIP` or `NodePort`
introduces another step, and means that users must have access beyond just the
control plane. Our goal here is for any user that can access the Kubernetes API
to be able to push images to the registry (given proper RBAC), without needing
any additional information about how to access it.

Let's take a step back and think about what actually happens when you deploy a
`Pod` on Kubernetes. First, you connect to the API server and give it your
manifest, which essentially contains instructions and data for running a process
on a `Node`. A simple manifest for our registry `Pod` might look something like
this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: k8scr
spec:
  containers:
    - name: k8scr
      image: hasheddan/k8scr-distribution:latest
```

> The `hasheddan/k8scr-distribution` image just contains an
> `http.ListenAndServe("80")` with a single handler function on the root path
> (`/`) that calls
> [go-containerregistry](https://github.com/google/go-containerregistry/blob/0233fcda5d536d4548d66e24829000e64a3e6059/pkg/registry/registry.go#L76)
> `registry.New()`, which is a very simple in-memory registry implementation.
> The source for this image can be found [here]().

We are giving Kubernetes a lot of flexibility with this manifest as we are not
specifying where it should run, what resources it requires, or how to access it.
We could certainly be [much more
overbearing](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/),
but hey, Kubernetes is pretty responsible, we trust it make smart decisions
here[^6]. The power of abstraction is that we can create functional and useful
software without knowing every detail about what is happening under the
hood[^7]. However, as soon as we want to access a workload, we suddenly care a
lot about where it is running and how it is exposed. Thus, we frequently give
Kubernetes more information so that we can declaratively tell it how we intend to
access the workloads it schedules in the future. As mentioned before, we might
do this with a `Service`.

The alternative to having Kubernetes make something available to us so we can
access it directly is to ask Kubernetes to broker communication between us and
the workload every time we want to talk to it. After all, we know how to talk to
the API server, and it keeps track of where our workload is running, so it can
always facilitate communication between the two of us. It normally would not
make any sense to have Kubernetes serve as a broker for all communication, but
there are two aspects of our scenario that make this strategy viable:

1. We are fine with conflating access to the control plane with access to the
   registry. For almost all other workloads, this is not the case. For example,
   Twitter does not expect that all users of its [Developer
   API](https://developer.twitter.com/en) will also have access to its
   Kubernetes control plane[^8].
2. We do not expect excessive traffic to the registry from outside the cluster.
   We have a limited set of users that will likely be pushing to the registry
   relatively infrequently, and we are not sensitive to any latency incurred by
   the overhead of using the API server as a proxy.

With these parameters established, how do we actually make this work? First, we
need to understand how we can ask the API server to pass along our requests to
the registry. It is sometimes easy to forget that all of the Kubernetes objects
that we `kubectl apply` are really just payloads to a [REST
API](https://en.wikipedia.org/wiki/Representational_state_transfer). Some of the
API endpoints that are not declarative are more easily forgotten, such as the
`Pod` [proxy
subresource](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.21/#-strong-proxy-operations-pod-v1-core-strong-).
It essentially allows us to send a request to the API server and ask that it be
forwarded to a `Pod` in the cluster. Once again, Kubernetes knows where that
`Pod` is and how to access it, so we can defer that responisbility to it. You
have likely seen this in practice with `kubectl port-forward`, and we can indeed
use this to `docker push` to our registry `Pod`.

```
$ kubectl port-forward pods/k8scr 8080:80 &
$ docker push 127.0.0.1:8080/hasheddan/cool-image:latest
```

So are we done? We could be! This accomplishes our goal of pushing to a registry
through the Kubernetes API server. However, it still requires us to port-forward
and specify `localhost` as our registry. Ideally, we could accomplish this
without having to do either one of those operations. However, instructing Docker
to natively route all requests through the API server is challenging due to the
fact that we cannot mangle paths. This is for good reason, as the [OCI
distribution
specification](https://github.com/opencontainers/distribution-spec/blob/main/spec.md)
is fairly prescriptive about the structure of a registry API. For instance, for
a registry to technically be compliant with the spec, it must offer the exact
paths it dictates (e.g. `/v2/<name>/manifests/<reference>`, etc.).

How are we going to fix this? Brace yourselves... we are going to write our own
client. While this may seem daunting, it really won't be with a little help from
[client-go](https://github.com/kubernetes/client-go) and
[go-containerregistry](https://github.com/google/go-containerregistry). In fact,
when we are done, we can package this up into a nice `kubectl` plugin and use it
to push to any registry `Pod` in your cluster.

## Loading our Image

The first thing we are going to need to do is load the image we want to push
from our Docker daemon. `go-containerregistry` makes this fairly straightforward
with its `daemon`
[source](https://github.com/google/go-containerregistry/tree/main/pkg/v1/daemon).

```
package main

func main() {
    daemon.Image
}
```

---

### Registry Authentication

If I do manage to set up the registry so that users are able to connect to it,
they will then need to be able to authenticate to it. Typically, a tool like
Docker will use credentials, or invoke a [credentials
helper](https://docs.docker.com/engine/reference/commandline/login/#credential-helpers),
to authenticate to a given registry. Our registry is only for internal usage,
meaning that there will not be anyone pushing to it who cannot also access the
Kubernetes cluster it is running in. With that restriction in place, managing an
additional authentication system becomes duplicative.


### Registry Authorization

Registries also typically allow for authorizing access to repositories (i.e.
user X can push to repository Y). As a fairly large caveat, we will not require
multi-tenant access (i.e. it's fine for anyone who can push to our registry to
have access to push to any repository in our registry), so we can defer


[^1]: Can we talk for a minute? You'll notice I used the term "container
registry" in the title of this post. Why did I do this? Well, it appears the
industry has decided that HTTP servers that you can pull and push OCI images to
and from are called container registries (e.g. [Google Container Registry
(GCR)](https://cloud.google.com/container-registry), [Elastic Container Registry
(ECR)](https://aws.amazon.com/ecr/), [Azure Container Registry
(ACR)](https://azure.microsoft.com/en-us/services/container-registry/)). Why do
we do this? Last time I checked there are no "containers" in these registries.
There are _images_ in this registry, and we can use those images as instructions
to create a conceptual "container" using
[namespaces](https://man7.org/linux/man-pages/man7/namespaces.7.html) and
[cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html). Since I am
clearly bothered by this, a very fair question you may be asking yourself is
"why did he use the term in the title of this post?". Your answer, dear reader,
is that he is but a mere pion in the vast univere of the software industry, and
he has been slowly ground down by the corporate world to conform to whatever
terminology they ruthlessly impose upon him... jk he did it for the Hacker News
clicks ¯\_(ツ)_/¯ [^2].

[^2]: Oh no... is this a footnote in a footnote? You can bet your bottom dollar
it is! Just thought I would clarify that this post almost certainly will get no
engagement on Hacker News, but I'm glad you decided to read it!

[^3]: I am thinking about labelling my posts from now on with a "snark score".
Feels like the way we are starting off this one could lead to some dangerously
high levels.

[^4]: Typically when you need to get images into a Kubernetes cluster and you
don't want to pull them from a registry, you [pre-pull them on each
`Node`](https://kubernetes.io/docs/concepts/containers/images/#pre-pulled-images)
and specify an `imagePullPolicy: Never`. However, this is only suitable for
cases where you are going through the `Node`'s container runtime to access an
image, and as I have mentioned in [previous
posts](https://danielmangum.com/posts/config-map-oci-image-cache/), sometimes it
makes sense to manage images [independent of the `Node` you are running
on](https://crossplane.io/docs/v1.2/concepts/packages.html).

[^5]: The GKE documentation on control plane access modes gives a [nice
summary](https://cloud.google.com/kubernetes-engine/docs/concepts/private-cluster-concept#overview)
of different ways to configure `kubectl` access.

[^6]: DISCLAIMER: you should never trust Kubernetes to make smart decisions for
you.

[^7]: I believe this to be true, but this sentence was terrifying to write.

[^8]: What a fun world that would be!